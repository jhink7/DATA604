{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data620 Week 10 Assignment\n",
    "\n",
    "## Justin Hink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1, load in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1813\n",
      "Not spam: 2788\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import spam dataset\n",
    "spam = pd.read_csv(\"spambase.csv\")\n",
    "\n",
    "spam_count = len(spam[spam.spamclass == 1])\n",
    "ham_count = len(spam[spam.spamclass == 0])\n",
    "\n",
    "print \"Spam: %d\" %spam_count\n",
    "print \"Not spam: %d\" %ham_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2, split data into training, test and eval data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 3220\n",
      "Eval set count: 690\n",
      "Testing set count: 691\n"
     ]
    }
   ],
   "source": [
    "prop_train = 0.7\n",
    "prop_val = 0.15\n",
    "prop_test = 0.15\n",
    "total_count = len(spam)\n",
    "trainNum = int(prop_train * total_count)\n",
    "evalNum = int(prop_val * total_count)\n",
    "testNum = total_count - trainNum - evalNum\n",
    "\n",
    "trainSet, testSet = train_test_split(spam, test_size=testNum, random_state=9)\n",
    "trainSet, evalSet = train_test_split(trainSet, test_size=evalNum, random_state=99)\n",
    "\n",
    "print \"Training set count: %d\" %len(trainSet)\n",
    "print \"Eval set count: %d\" %len(evalSet)\n",
    "print \"Testing set count: %d\" %len(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Method 1 - Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "true positives: 1263\n",
      "false positives: 0\n",
      "true negatives: 1946\n",
      "false negatives: 11\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       1.00      0.99      1.00      1274\n",
      "        Ham       0.99      1.00      1.00      1946\n",
      "\n",
      "avg / total       1.00      1.00      1.00      3220\n",
      "\n",
      "\n",
      "true positives: 238\n",
      "false positives: 13\n",
      "true negatives: 414\n",
      "false negatives: 26\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.95      0.90      0.92       264\n",
      "        Ham       0.94      0.97      0.96       427\n",
      "\n",
      "avg / total       0.94      0.94      0.94       691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "# utility method to print model metrics\n",
    "def print_model_metrics(actual, pred):\n",
    "    cm = sm.confusion_matrix(actual, pred, labels=[1, 0])\n",
    "\n",
    "    print\n",
    "    print \"true positives: %d\" %cm[0,0]\n",
    "    print \"false positives: %d\" %cm[1,0]\n",
    "    print \"true negatives: %d\" %cm[1,1]\n",
    "    print \"false negatives: %d\" %cm[0,1]\n",
    "    print\n",
    "    print sm.classification_report(actual, pred, labels=[1,0], target_names=[\"Spam\", \"Ham\"])\n",
    "\n",
    "yTest = testSet['spamclass']\n",
    "xTest = testSet.drop(labels='spamclass', axis=1)\n",
    "yTrain = trainSet['spamclass']\n",
    "xTrain = trainSet.drop(labels='spamclass', axis=1)\n",
    "\n",
    "rf = ensemble.RandomForestClassifier(criterion=\"entropy\", random_state=99)\n",
    "m1 = rf.fit(xTrain, yTrain)\n",
    "\n",
    "# check our in sample model efficacy\n",
    "train1 = m1.predict(xTrain)\n",
    "print_model_metrics(yTrain, train1)\n",
    "\n",
    "# check out of sample model efficacy\n",
    "test1 = m1.predict(xTest)\n",
    "print_model_metrics(yTest, test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 - SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "true positives: 1139\n",
      "false positives: 42\n",
      "true negatives: 1904\n",
      "false negatives: 135\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.96      0.89      0.93      1274\n",
      "        Ham       0.93      0.98      0.96      1946\n",
      "\n",
      "avg / total       0.95      0.95      0.94      3220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVC(random_state=99)\n",
    "m2 = sv.fit(xTrain, yTrain)\n",
    "\n",
    "sv_train = m2.predict(xTrain)\n",
    "print_model_metrics(yTrain, sv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Both the random forest and svm methods were able to classify the spam with a high degree of effectiveness.  However, because of the application (email spam), we would choose the SVM method as it had a smaller number of false positives (ie - non spam emails that are flagged as spam).  We have greater sensitivity to improperly blocking good emails than we have for falsely allowing spam through the filter.\n",
    "\n",
    "Also of minor note:  Both models performed much better when evaluated in sample (as expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
